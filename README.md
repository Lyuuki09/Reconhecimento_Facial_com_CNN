# ğŸš€ Sistema de Reconhecimento Facial Inteligente com PyTorch ğŸ§ 

---

## âœ¨ VisÃ£o Geral do Projeto

Este projeto tem como objetivo principal desenvolver um sistema robusto de reconhecimento facial utilizando as capacidades avanÃ§adas do PyTorch e Redes Neurais Convolucionais (CNNs). AlÃ©m do reconhecimento padrÃ£o, o sistema incorpora uma funcionalidade inovadora de **cadastro incremental de usuÃ¡rios**, permitindo a inclusÃ£o de novas identidades sem a necessidade de retreinar o modelo do zero.

### ğŸ¯ Objetivo Principal (do Professor)

Desenvolver um sistema de reconhecimento facial utilizando PyTorch e redes neurais convolucionais (CNN). O foco do projeto Ã© aplicar CNNs, com ou sem transfer learning, para identificaÃ§Ã£o de pessoas a partir de imagens de rosto.

### ğŸ“œ Regras e RestriÃ§Ãµes TÃ©cnicas (do Professor)

- O projeto deve ser implementado com PyTorch.
- A arquitetura deve obrigatoriamente utilizar redes convolucionais (CNN).
- Ã‰ permitido o uso de modelos prÃ©-treinados como ResNet, VGGFace, FaceNet, EfficientNet, etc.
- SÃ£o aceitas tanto abordagens de classificaÃ§Ã£o direta (com imagens de rostos recortados) quanto de detecÃ§Ã£o de objetos, como YOLO.
- Pode-se usar bibliotecas como OpenCV, MTCNN, MediaPipe, ou YOLO para detecÃ§Ã£o facial.
- A entrada do modelo pode ser a imagem completa ou o recorte facial, conforme a abordagem adotada.

### âœ… Requisitos Funcionais (do Professor)

- O sistema deve reconhecer corretamente cada participante com base em uma imagem contendo seu rosto.
- A entrada pode ser uma imagem com um ou mais rostos.
- A saÃ­da deve indicar o nome ou rÃ³tulo da pessoa reconhecida.

---

## ğŸ› ï¸ Detalhes da ImplementaÃ§Ã£o

Aqui, detalhamos o coraÃ§Ã£o tÃ©cnico do nosso sistema.

### ğŸ—ï¸ Arquitetura da Rede Neural

Nosso sistema utiliza uma **Rede Neural Convolucional (CNN)** baseada na arquitetura prÃ©-treinada **ResNet18**. A ResNet18 Ã© conhecida por sua eficiÃªncia e capacidade de aprender representaÃ§Ãµes hierÃ¡rquicas complexas de imagens.

- **Backbone**: ResNet18 prÃ©-treinada em ImageNet. As camadas iniciais (backbone) sÃ£o congeladas durante o fine-tuning para preservar o conhecimento de caracterÃ­sticas gerais.
- **Embedding Head**: Uma camada convolucional e/ou linear adicional (`nn.Sequential` em `ImprovedFaceRecognitionModel`) que mapeia as features extraÃ­das pelo backbone para um vetor de embedding de tamanho `EMBEDDING_SIZE` (512 no nosso caso). Este embedding serve como uma representaÃ§Ã£o numÃ©rica Ãºnica do rosto.
- **Classificador**: Uma camada linear final que mapeia o vetor de embedding para o nÃºmero de classes (pessoas) presentes no dataset. Esta camada Ã© adaptada dinamicamente quando novos usuÃ¡rios sÃ£o cadastrados.

### âš™ï¸ EstratÃ©gia de PrÃ©-processamento e Treinamento

Nosso pipeline de treinamento Ã© cuidadosamente projetado para otimizar o aprendizado e a adaptabilidade do modelo.

#### PrÃ©-processamento de Imagens

As imagens passam por um pipeline de prÃ©-processamento robusto:
1.  **Redimensionamento**: Todas as imagens sÃ£o redimensionadas para `224x224` pixels, o tamanho de entrada esperado pela ResNet18.
2.  **ConversÃ£o para Tensor**: Imagens sÃ£o convertidas para tensores PyTorch.
3.  **NormalizaÃ§Ã£o**: Aplicamos a normalizaÃ§Ã£o padrÃ£o da ImageNet (`mean=[0.485, 0.456, 0.406]`, `std=[0.229, 0.224, 0.225]`) para alinhar a distribuiÃ§Ã£o dos pixels com os dados em que o modelo foi prÃ©-treinado.
4.  **Carregamento de Imagens Seguras**: Para lidar com caminhos de arquivo que contenham caracteres especiais (comuns no Windows), utilizamos a biblioteca `Pillow` (`PIL.Image.open`) para carregar as imagens, que Ã© mais robusta que `cv2.imread` para esses casos.

#### Treinamento do Modelo

-   **Otimizador**: `AdamW` Ã© utilizado como otimizador, conhecido por sua boa performance e tratamento de decaimento de peso.
-   **Taxa de Aprendizado (Learning Rate)**: `LEARNING_RATE = 0.0001` para o treinamento inicial.
-   **Agendador de Taxa de Aprendizado (Scheduler)**: `ReduceLROnPlateau` monitora a acurÃ¡cia de validaÃ§Ã£o. Se a acurÃ¡cia nÃ£o melhorar por um determinado nÃºmero de Ã©pocas (`patience`), a taxa de aprendizado Ã© reduzida, ajudando o modelo a convergir.
-   **CritÃ©rio de Perda (Loss Function)**: `nn.CrossEntropyLoss` Ã© usada para classificaÃ§Ã£o multi-classe.
-   **Ã‰pocas**: Definido como `EPOCHS = 1` para testes rÃ¡pidos. Para um modelo robusto, um nÃºmero maior de Ã©pocas (e.g., 20-50) seria necessÃ¡rio.
-   **Early Stopping**: Um mecanismo de early stopping com `patience = 15` monitora a acurÃ¡cia de validaÃ§Ã£o para evitar overfitting, parando o treinamento se a performance nÃ£o melhorar.

### ğŸ“ˆ Resultados do Treinamento

ApÃ³s o treinamento, o sistema salva o modelo com a melhor acurÃ¡cia de validaÃ§Ã£o. Os resultados incluem:

-   **AcurÃ¡cia de Treinamento e ValidaÃ§Ã£o**: Monitoradas ao longo das Ã©pocas.
-   **Perda de Treinamento**: Monitorada para verificar a convergÃªncia.
-   **Matriz de ConfusÃ£o**: Uma matriz de confusÃ£o Ã© gerada e salva (ou exibida) para visualizar o desempenho do classificador em cada classe, identificando erros e acertos.

---

## â• Adicional: MVP para Cadastro Incremental de UsuÃ¡rios

Este Ã© um diferencial crucial do nosso projeto, permitindo a **evoluÃ§Ã£o contÃ­nua** do sistema.

### ğŸ¯ Objetivo (do Professor)

Permitir a inclusÃ£o de novos usuÃ¡rios no sistema de reconhecimento facial sem apagar ou sobrescrever os usuÃ¡rios jÃ¡ cadastrados.

### ğŸ“‹ Requisitos TÃ©cnicos (do Professor)

- O sistema deve armazenar as representaÃ§Ãµes faciais dos usuÃ¡rios (por exemplo, vetores de embeddings ou imagens associadas aos rÃ³tulos).
    - **Nossa Abordagem**: Armazenamos as imagens diretamente em diretÃ³rios nomeados e o modelo aprende os embeddings internamente.
- O cadastro pode ser feito a partir de uma ou mais imagens do novo usuÃ¡rio.
    - **Nossa Abordagem**: A interface permite upload de mÃºltiplas imagens.
- O modelo pode:
    - Atualizar um banco de embeddings, se a arquitetura utilizar essa abordagem.
    - Utilizar fine-tuning leve ou incremental em uma rede existente, se necessÃ¡rio.
        - **Nossa Abordagem**: Utilizamos fine-tuning leve.
    - Atualizar um banco de dados com imagens e rÃ³tulos, e reprocessar conforme necessÃ¡rio.
        - **Nossa Abordagem**: As imagens sÃ£o copiadas para as pastas de `Fotos/` e o mapeamento de labels (`label_mapping_fixed.json`) Ã© atualizado.

### ğŸš€ Funcionalidades Implementadas (no MVP)

-   **Interface GrÃ¡fica Simples (Streamlit)**: Uma seÃ§Ã£o dedicada no `app.py` permite que o usuÃ¡rio insira o nome da nova pessoa e faÃ§a upload de suas imagens.
-   **AssociaÃ§Ã£o Nome/RÃ³tulo**: As imagens sÃ£o salvas em uma pasta com o nome do usuÃ¡rio, e o sistema automaticamente associa este nome a uma nova classe/rÃ³tulo.
-   **Fine-tuning Leve**:
    1.  O modelo prÃ©-treinado Ã© carregado.
    2.  Se houver uma nova classe (novo usuÃ¡rio), a camada classificadora final do modelo Ã© adaptada para incluir essa nova classe.
    3.  As camadas do **backbone (ResNet18) sÃ£o congeladas**. Isso impede que os pesos das caracterÃ­sticas gerais sejam alterados.
    4.  Apenas o **embedding head e a nova camada classificadora sÃ£o treinados** (descongelados) com uma taxa de aprendizado menor. Isso acelera o treinamento e foca na adaptaÃ§Ã£o Ã s novas identidades.
-   **Garantia de Reconhecimento Anterior**: A estratÃ©gia de fine-tuning leve minimiza o impacto no reconhecimento de usuÃ¡rios jÃ¡ cadastrados, pois o conhecimento prÃ©-existente do backbone Ã© preservado. Testes prÃ¡ticos apÃ³s a inclusÃ£o de novos usuÃ¡rios sÃ£o recomendados para validar isso.
-   **Registro Persistente**: O modelo treinado (`face_recognition_model_fixed.pt`) e o mapeamento de labels (`label_mapping_fixed.json`) sÃ£o salvos e atualizados apÃ³s cada processo de fine-tuning, garantindo que as informaÃ§Ãµes dos novos usuÃ¡rios sejam mantidas.

---

## âš™ï¸ ConfiguraÃ§Ã£o do Ambiente

1.  **Clone este repositÃ³rio:**
    ```bash
    git clone [URL_DO_REPOSITORIO]
    cd [NOME_DO_DIRETORIO]
    ```

2.  **Instale as dependÃªncias:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Para o treinamento inicial do modelo:**
    (Este passo Ã© necessÃ¡rio uma vez para gerar o modelo base antes de usar a interface Streamlit)
    ```bash
    python rooney_trabalho.py
    ```

---

## ğŸš€ Como Usar a AplicaÃ§Ã£o

1.  **Inicie a aplicaÃ§Ã£o Streamlit:**
    ```bash
    streamlit run app.py
    ```
    A interface web serÃ¡ aberta automaticamente no seu navegador padrÃ£o.

2.  **Funcionalidades na Interface:**
    -   **Reconhecimento Facial**: FaÃ§a upload de uma imagem contendo um rosto para que o sistema tente identificÃ¡-lo.
    -   **Cadastro Incremental de Novos UsuÃ¡rios**:
        1.  Na seÃ§Ã£o "Cadastro Incremental de Novos UsuÃ¡rios", digite o **Nome do Novo UsuÃ¡rio**.
        2.  FaÃ§a upload de **pelo menos 5 imagens** do novo usuÃ¡rio (quanto mais imagens de boa qualidade, melhor a performance).
        3.  Clique em "**Cadastrar Novo UsuÃ¡rio e Fine-tune**".
        4.  Aguarde o processo de fine-tuning (pode levar alguns minutos).
        5.  **Importante**: ApÃ³s o cadastro, **recarregue a pÃ¡gina do Streamlit** no seu navegador para que o novo modelo (com as novas classes) seja carregado e utilizado para reconhecimento.

---

## ğŸ’¡ Dicas para Melhor Performance

-   **Qualidade das Imagens**: Use imagens de boa qualidade para o cadastro e reconhecimento.
-   **NÃºmero de Imagens**: ForneÃ§a pelo menos 5-10 imagens distintas por usuÃ¡rio para o cadastro (quanto mais, melhor).
-   **IluminaÃ§Ã£o e PosiÃ§Ã£o**: Certifique-se de que as imagens tenham boa iluminaÃ§Ã£o e que o rosto esteja bem visÃ­vel.
-   **Recorte Facial**: Embora o sistema lide com redimensionamento, imagens com rostos bem enquadrados e centralizados tendem a performar melhor.

---

## â‰ï¸ SoluÃ§Ã£o de Problemas Comuns

1.  **`ModuleNotFoundError`**: Certifique-se de que todas as dependÃªncias em `requirements.txt` estÃ£o instaladas e que o ambiente virtual estÃ¡ ativado. Renomeie `Rooney trabalho.py` para `rooney_trabalho.py` se ainda nÃ£o o fez.
2.  **`expected scalar type Double but found Float`**: Execute `python rooney_trabalho.py` para retreinar o modelo com a precisÃ£o correta antes de rodar `streamlit run app.py`.
3.  **Erro ao Carregar Imagens (`cv2.imread` falha com caracteres especiais)**: A soluÃ§Ã£o com `Pillow` foi implementada. Verifique se as imagens nÃ£o estÃ£o corrompidas.
4.  **Erro de CUDA**: Se vocÃª nÃ£o tiver uma GPU NVIDIA, o sistema funcionarÃ¡ em CPU, mas serÃ¡ mais lento. Certifique-se de que as bibliotecas `torch` e `torchvision` foram instaladas para CPU se nÃ£o tiver CUDA.
5.  **Webcam nÃ£o funciona**: Certifique-se de que sua webcam estÃ¡ funcionando e acessÃ­vel por outros aplicativos. PermissÃµes do navegador podem ser necessÃ¡rias. (Se vocÃª adicionar a funcionalidade de webcam).
6.  **Erros de MemÃ³ria**: Se encontrar erros de memÃ³ria, tente reduzir o `BATCH_SIZE` em `rooney_trabalho.py` ou o nÃºmero de `num_workers` nos `DataLoader`s.

---

## ğŸ“‚ Estrutura de DiretÃ³rios

```
projeto/
â”œâ”€â”€ app.py                     # ğŸš€ Interface web principal (Streamlit)
â”œâ”€â”€ rooney_trabalho.py         # ğŸ§  Core do sistema: modelo, treinamento e fine-tuning
â”œâ”€â”€ requirements.txt           # ğŸ“¦ DependÃªncias do Python
â”œâ”€â”€ .gitignore                 # ğŸš« Arquivos e diretÃ³rios a serem ignorados pelo Git
â”œâ”€â”€ README.md                  # ğŸ“„ Este arquivo: documentaÃ§Ã£o do projeto
â””â”€â”€ data/                      # ğŸ—ƒï¸ DiretÃ³rio para dados do projeto
    â”œâ”€â”€ raw/                   # ğŸ“¸ Imagens originais dos usuÃ¡rios (mantÃ©m subpastas por pessoa)
    â”‚   â””â”€â”€ .gitkeep
    â”œâ”€â”€ processed/             # âœ¨ Imagens prÃ©-processadas (geradas, se aplicÃ¡vel)
    â”‚   â””â”€â”€ .gitkeep
    â””â”€â”€ models/                # ğŸ’¾ Modelos PyTorch treinados e mapeamentos
        â””â”€â”€ .gitkeep
```

---

## ğŸ¤ Contribuindo

Sinta-se Ã  vontade para abrir [issues]([URL_DO_REPOSITORIO]/issues) para relatar bugs ou sugerir melhorias, ou enviar [pull requests]([URL_DO_REPOSITORIO]/pulls) com novas funcionalidades.

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a [MIT License](https://opensource.org/licenses/MIT). 